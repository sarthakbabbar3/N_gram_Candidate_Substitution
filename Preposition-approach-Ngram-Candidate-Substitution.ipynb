{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from nltk import ngrams\n",
    "import sys\n",
    "sys.path.append('/home/gpuadmin/Candice/scripts/data-extraction')\n",
    "sys.path.append('/home/gpuadmin/projects/candice/AGEL/agel_backend/agel_v15/copy_editing/get_grammar_suggestions')\n",
    "root_path = Path('/home/gpuadmin/Candice/Data/extracted/raw_edit_pairs')\n",
    "from __parameters__ import word_tokenizer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.util import regexp_span_tokenize\n",
    "bracket_regex_pattern = r\"\"\"[\\(\\[].*?[\\)\\]]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "all_preps = set(['across', 'near', 'along', 'on', 'down', 'till', 'underneath', 'beneath', 'against', 'to', 'under', 'for',\n",
    "             'through', 'towards', 'above', 'of', 'until', 'between', 'by', 'among', 'during', 'after', 'over',\n",
    "             'toward', 'in', 'before', 'around', 'inside', 'up', 'from', 'into', 'outside', 'below', 'onto', 'beside',\n",
    "             'behind', 'at'] + ['aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'around', 'as', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'following', 'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'next', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'save', 'since', 'than', 'through', 'till', 'to', 'toward', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all data\n",
    "%time all_data = [(load_json(path), path.name) for path in root_path.rglob('*.json') if path.parent.name in ['2FA-2T-1358', '2F', '2T','2FA', 'CE ONLY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6346, 2721)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "len_all_data = len(all_data)\n",
    "train_frac = 0.7\n",
    "train_files = all_data[:int(train_frac*len_all_data)]\n",
    "test_files = all_data[int(train_frac*len_all_data):]\n",
    "len(train_files), len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_names = [name for _, name in test_files]\n",
    "train_file_names = [name for _, name in train_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BracketTokenizer(RegexpTokenizer):\n",
    "    def tokenize(self, text):\n",
    "        self._check_regexp()\n",
    "        # If our regexp matches gaps, use re.split:\n",
    "        if self._gaps:\n",
    "            if self._discard_empty:\n",
    "                return [tok for tok in self._regexp.split(text) if tok]\n",
    "            else:\n",
    "                return self._regexp.split(text)\n",
    "\n",
    "        # If our regexp matches tokens, use re.findall:\n",
    "        else:\n",
    "            return list(self._regexp.finditer(text))\n",
    "\n",
    "    def span_tokenize(self, text):\n",
    "        self._check_regexp()\n",
    "\n",
    "        if self._gaps:\n",
    "            for left, right in regexp_span_tokenize(text, self._regexp):\n",
    "                if not (self._discard_empty and left == right):\n",
    "                    yield left, right\n",
    "        else:\n",
    "            for m in re.finditer(self._regexp, text):\n",
    "                if re.match(bracket_regex_pattern, m.group(0)) is not None:\n",
    "                    yield {'span': m.span(), 'text': m.group(0), 'bracket_content': True}\n",
    "                else:\n",
    "                    yield {'span': m.span(), 'text': m.group(0), 'bracket_content': False}\n",
    "\n",
    "\n",
    "bracket_tokenizer = BracketTokenizer(r\"\"\"[\\(\\[].*?[\\)\\]]|[^\\(\\[\\)\\]]+\"\"\")\n",
    "\n",
    "def remove_brackets(text):\n",
    "    new_text = ''\n",
    "    for item in bracket_tokenizer.span_tokenize(text):\n",
    "        new_text += ' ' * len(item['text']) if item['bracket_content'] else item['text']\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 gram approach\n",
    "n = 9\n",
    "all_trigrams = []\n",
    "count = 0\n",
    "for sent_list, _ in train_files:\n",
    "    print(count)\n",
    "    count += 1\n",
    "    for dic in sent_list:\n",
    "        sentence = remove_brackets(dic['edit']['text'])\n",
    "        trigrams = ngrams(word_tokenizer.tokenize(sentence), n)\n",
    "        for gram in trigrams:\n",
    "            all_trigrams.append(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering n-grams that have a preposition in the middle.\n",
    "prep_trigrams = []\n",
    "for trigram in all_trigrams:\n",
    "    if trigram[len(trigram)//2] in all_preps:\n",
    "        prep_trigrams.append(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.6 s, sys: 199 ms, total: 53.8 s\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.lm import MLE\n",
    "lm = MLE(n)\n",
    "lm.fit([prep_trigrams], vocabulary_text=set([tok for item in prep_trigrams for tok in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('all_prep_lm.pickle', 'wb') as f:\n",
    "    pickle.dump(lm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible candidate substitutions as observed in our data.\n",
    "candidates = {'following' : ['after'], 'to': ['–', 'and'], '–': ['to'], 'as':['because'], 'among':['in'], 'since':['because'], 'upon': ['on'],\n",
    "'between': ['among'], 'of':['in']}\n",
    "\n",
    "#getting score for all possible candidates\n",
    "def get_best_score(trigram):\n",
    "    all_suggestions = [(trigram, lm.score(trigram[-1], trigram[:-1]))]\n",
    "    for cand in candidates[trigram[len(trigram)//2]]:\n",
    "        _temp = list(trigram)\n",
    "        _temp[len(trigram)//2] = cand\n",
    "        _temp = tuple(_temp)\n",
    "        all_suggestions.append((_temp, lm.score(_temp[-1], _temp[:-1])))\n",
    "    return max(all_suggestions, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "def update(item):\n",
    "    text = item['raw_text']\n",
    "    clean_text = remove_brackets(text)\n",
    "    tokens = word_tokenizer.tokenize(clean_text)\n",
    "    spans = list(word_tokenizer.span_tokenize(clean_text))\n",
    "    \n",
    "    if len(set(tokens).intersection(set(candidates.keys()))) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    trigram_toks = list(ngrams(tokens, n))\n",
    "    trigram_spans = list(ngrams(spans, n))\n",
    "    \n",
    "    #finding n-grams in sentence containing relevant preposition\n",
    "    possible_suggestions = [idx for idx, gram in enumerate(trigram_toks) if gram[len(gram)//2] in candidates.keys()]\n",
    "    final_suggestions = []\n",
    "    \n",
    "    for idx in possible_suggestions:\n",
    "        gram = trigram_toks[idx]\n",
    "        best_sugg, score = get_best_score(gram)\n",
    "        if best_sugg != gram: #if suggestion is different than the original preposition\n",
    "            final_suggestions.append((idx, best_sugg, score))\n",
    "\n",
    "    fp, tp, fn = 0, 0, 0\n",
    "#     Checking if the best suggestion is correct by comparing it to actual track changes made by editors\n",
    "    rel_mis = [mis for mis in item['track_changes'] if mis['insertion'] and mis['deletion']]\n",
    "    rel_mis = [mis for mis in rel_mis if len(mis['insertion']['tokens']) == 1 and len(clean_text[mis['deletion'][0]: mis['deletion'][1]].split(' ')) == 1]\n",
    "    rel_mis = [mis for mis in rel_mis if clean_text[mis['deletion'][0]: mis['deletion'][1]] in candidates.keys()]\n",
    "\n",
    "    for mis in rel_mis:\n",
    "        flag = False\n",
    "        for idx, sugg, _ in final_suggestions:\n",
    "            # If the deleted word is same\n",
    "            if tuple(mis['deletion']) == trigram_spans[idx][len(trigram_spans[idx])//2]:\n",
    "                flag = True\n",
    "                #if the suggestion is the same as the one made by editor\n",
    "                if sugg[len(sugg)//2] == mis['insertion']['tokens'][0]:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "\n",
    "        fn += 1 if not flag else 0\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_text': 'The KHSC Palliative Care Consult Service was involved in the care of 14.2%        of patients in this study. All of them had a documented DNR and underwent device deactivation prior to death as part of their end-of-life care plan. The mean time to device deactivation for patients with Palliative Care consultation following establishment of DNR status was 7 days. Six of those seven patients had their device deactivated within one day of a DNR order being put in place, and the seventh patient had device deactivation 43 days after their DNR order was instituted. Patients without palliative care involvement had their device deactivated a mean of 79 days after their DNR order was instituted.',\n",
       " 'track_changes': [{'deletion': [176, 184],\n",
       "   'insertion': {'tokens': ['before'], 'position': 184}},\n",
       "  {'deletion': [315, 324],\n",
       "   'insertion': {'tokens': ['after'], 'position': 324}},\n",
       "  {'deletion': [378, 383], 'insertion': {'tokens': ['7'], 'position': 383}},\n",
       "  {'deletion': [429, 432], 'insertion': {'tokens': ['1'], 'position': 432}}],\n",
       " 'path': '/home/gpuadmin/Candice/Data/extracted/ce_non_bracket_lcs/2T/AJC24050_cedited.json'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mistakes[2937]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relevant_mistake_loader import LoadRelevantMistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_loader = LoadRelevantMistakes(path_to_mistakes='/home/gpuadmin/Candice/Data/extracted/ce_non_bracket_lcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-17 16:25:20,874 :: INFO :: relevant_mistake_loader :: 2FA-2T-1358 track change files = 16\n",
      "2019-12-17 16:25:20,875 :: INFO :: relevant_mistake_loader :: 2F track change files = 41\n",
      "2019-12-17 16:25:20,876 :: INFO :: relevant_mistake_loader :: 2T track change files = 6029\n",
      "2019-12-17 16:25:20,877 :: INFO :: relevant_mistake_loader :: 2FA track change files = 1890\n",
      "2019-12-17 16:25:20,878 :: INFO :: relevant_mistake_loader :: CE ONLY track change files = 452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.8 s, sys: 485 ms, total: 8.28 s\n",
      "Wall time: 8.26 s\n"
     ]
    }
   ],
   "source": [
    "%time all_mistakes = mistake_loader.extract_all_mistakes(file_types=['2FA-2T-1358', '2F', '2T','2FA', 'CE ONLY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186461"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_mistakes = [mis for mis in all_mistakes if Path(mis['path']).name in test_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rel_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 0 ns, total: 25.2 s\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "#Calculating accuracy on test files\n",
    "%%time\n",
    "tp, fp, fn = 0, 0, 0\n",
    "for item in rel_mistakes:\n",
    "    _tp, _fp, _fn = update(item)\n",
    "#     if _fn>0:\n",
    "#         print(item)\n",
    "#         break\n",
    "    tp += _tp\n",
    "    fp += _fp\n",
    "    fn += _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-associated disparities in parent use of Internet and cell phone technologies exist, but parents’ desire for use of these technologies for provider communication was overall high and did not differ by health literacy'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Conclusions: Health literacy-associated disparities in parent use of Internet and cell phone technologies exist, but parents’ desire for use of these technologies for provider communication was overall high and did not differ by health literacy\"[28:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 2, 9247)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 2, 9247)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tp/(fp+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002373503074765347"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp/(tp + fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
